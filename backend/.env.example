# Mode de fonctionnement: "api" (Mistral Cloud) ou "local" (vLLM)
LLM_MODE=local

# Database Configuration
DB_NAME=
DB_USER=
DB_PASSWORD=
DB_HOST=
DB_PORT=
DATABASE_URL=

# Security
JWT_SECRET_KEY=xxx
DEFAULT_ADMIN_EMAIL=admin@example.com
DEFAULT_ADMIN_TRIGRAMME=ADM
DEFAULT_ADMIN_PASSWORD=change-me
ADMIN_TRIGRAMMES=ADM

# Redis
REDIS_URL=redis://localhost:6379/0

# ========================================
# Mode API (Mistral Cloud)
# ========================================
# Required if LLM_MODE=api
MISTRAL_API_KEY=x
MISTRAL_MODEL=mistral-small-latest
VISION_MODEL=pixtral-large-latest
# Optional: override endpoint/clé pour un autre VLM compatible OpenAI
# VISION_API_URL=https://api.mistral.ai/v1/chat/completions
# VISION_API_KEY=${MISTRAL_API_KEY}

# ========================================
# Mode Local (vLLM)
# ========================================
# Required if LLM_MODE=local
# Utilisez une adresse accessible côté client (ex: http://localhost:5263/...)
VLLM_API_URL=http://localhost:5263/v1/chat/completions
VLLM_MODEL_NAME=Mistral-Small-3.1-24B-Instruct-2503
VLLM_MAX_TOKENS=6000
VLLM_TEMPERATURE=0.0
VLLM_TIMEOUT=500
VISION_VLLM_URL=http://localhost:8085/v1/chat/completions
VISION_VLLM_MODEL=pixtral-large-latest

# ========================================
# Application Settings
# ========================================
APP_NAME=FoyerGPT Backend
APP_VERSION=1.0.0
DEBUG=false

# ========================================
# Configuration simplifiée des URLs
# ========================================
# Définissez simplement les URLs complètes, le système extraira automatiquement host et port

# URL complète du backend (ex: http://foyergpt.lefoyer.lu:8077)
BACKEND_URL=http://localhost:8077

# URL complète du frontend (ex: http://foyergpt.lefoyer.lu:8091)
FRONTEND_URL=http://localhost:8091

# Host d'écoute pour le serveur (généralement 0.0.0.0 pour accepter toutes les connexions)
SERVER_HOST=0.0.0.0

# CORS Origins (généré automatiquement depuis les URLs)
# Peut être surchargé si nécessaire:
# CORS_ORIGINS_OVERRIDE=

# Session
SESSION_EXPIRE_HOURS=24

# Storage
STORAGE_PATH=./storage
MAX_FILE_SIZE_MB=10
MAX_DOCUMENTS_PER_AGENT=10
MAX_DOCUMENTS_PER_CHAT=5

# PDF Processing
PDF_USE_VISION_THRESHOLD=100
PDF_MAX_PAGES_VISION=10

# Embeddings
EMBEDDING_PROVIDER=mistral  # automatique si LLM_MODE=api
EMBEDDING_MODEL=mistral-embed
# EMBEDDING_LOCAL_MODEL_PATH=/home/llama/models/base_models/bge-reranker-large (obligatoire si EMBEDDING_PROVIDER=local)

RAG_ONLY=true
