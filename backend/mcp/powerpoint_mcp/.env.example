# Mistral API Configuration
MISTRAL_API_KEY=your-api-key-here
MISTRAL_MODEL=mistral-small-latest
MISTRAL_TEMPERATURE=0.3
MISTRAL_MAX_TOKENS=128000

# Mode: api or local
MISTRAL_MODE=api

# Local LLM Configuration (for VLLM)
LOCAL_BASE_URL=http://localhost:5263/v1
LOCAL_MODEL_PATH=/home/llama/models/base_models/Mistral-Small-3.1-24B-Instruct-2503

# Output Configuration
OUTPUT_PRETTY_JSON=true
OUTPUT_INDENT=2
OUTPUT_DIR=output

# Logging
LOG_LEVEL=INFO
DEBUG=false