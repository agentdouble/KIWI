## LLM API Configuration (OpenAI-compatible, e.g. Mistral)
API_URL=https://api.mistral.ai/v1/chat/completions
API_KEY=your-api-key-here
API_MODEL=mistral-small-latest
MISTRAL_TEMPERATURE=0.3
MISTRAL_MAX_TOKENS=128000

## Mode: api or local
MISTRAL_MODE=api

## Local LLM Configuration (for VLLM)
LOCAL_BASE_URL=http://localhost:5263/v1
LOCAL_MODEL_PATH=/home/llama/models/base_models/Mistral-Small-3.1-24B-Instruct-2503

## Output Configuration
OUTPUT_PRETTY_JSON=true
OUTPUT_INDENT=2
OUTPUT_DIR=output

## Logging
LOG_LEVEL=INFO
DEBUG=false
